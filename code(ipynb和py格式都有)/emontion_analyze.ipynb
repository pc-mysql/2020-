{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%导入库和定义初始化\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-f342c4c10d02>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0munittest\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mgensim\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mcorpora\u001B[0m      \u001B[1;31m#\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mwordcloud\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mWordCloud\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mwordcloud\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mgensim\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "from gensim import corpora      #\n",
    "from wordcloud import WordCloud, wordcloud\n",
    "import gensim\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import stopwords as stopwords\n",
    "import pandas as pd\n",
    "\n",
    "# Matplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Scikit-learn\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import Word2Vec, LdaModel\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "W2V_SIZE = 300\n",
    "W2V_WINDOW = 7\n",
    "W2V_EPOCH = 32\n",
    "W2V_MIN_COUNT = 10\n",
    "\n",
    "# KERAS\n",
    "SEQUENCE_LENGTH = 300\n",
    "EPOCHS = 8\n",
    "BATCH_SIZE = 1024\n",
    "POSITIVE = \"POSITIVE\"\n",
    "NEGATIVE = \"NEGATIVE\"\n",
    "NEUTRAL = \"NEUTRAL\"\n",
    "SENTIMENT_THRESHOLDS = (0.4, 0.7)\n",
    "# Keras\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from  nltk.stem import SnowballStemmer\n",
    "import collections\n",
    "\n",
    "# Word2vec\n",
    "\n",
    "# Utility\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess(text):           #训练可以加入别的东西，自己选择\n",
    "    tokens = []\n",
    "    words = text.split(' ')#也就是这个东西如果不是在stop_words中的话，进行一部分操作\n",
    "    for token in words:  # for token in words,也就是如果不存在一个东西可以使用，\n",
    "        token = token.lower()\n",
    "        if token not in stop_words:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "    return \" \".join(tokens)\n",
    "    #得到了一堆列表的列表\n",
    "#导入数据\n",
    "hair = pd.read_csv(\"hair_dryer.tsv\", sep=\"\\t\", )        #\n",
    "microwave = pd.read_csv(\"microwave.tsv\", sep=\"\\t\")   #\n",
    "pac = pd.read_csv(\"pacifier.tsv\", sep=\"\\t\")  #\n",
    "#转换时间类型以及给予标记\n",
    "hair['review_date'] = pd.to_datetime(hair['review_date'],format=\"%m/%d/%Y\")  #也就是得到了date的时间\n",
    "pac['review_date'] = pd.to_datetime(pac['review_date'],format=\"%m/%d/%Y\")   #得到pac的时间\n",
    "microwave['review_date'] = pd.to_datetime(microwave['review_date'],format=\"%m/%d/%Y\")\n",
    "stop_words = stopwords.words(\"english\") #记录停顿词\n",
    "wnl = nltk.WordNetLemmatizer()  #\n",
    "stemmer = SnowballStemmer(\"english\")  #也就是\n",
    "tokens = []\n",
    "#对hair进行处理\n",
    "hair['review_body'] = hair['review_body'].apply(lambda x:preprocess(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%文本预处理函数\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "word_string=''\n",
    "for i in [_test for _test in hair.review_body]:\n",
    "    word_string+=str(i)\n",
    "w = wordcloud.WordCloud(width=1000,height=700,background_color='white',font_path='msyh.ttc',max_words=50,collocations=True)\n",
    "w.generate(word_string)\n",
    "# 将生成的词云保存为temp.png图片文件，保存到当前文件夹中\n",
    "w.to_file('temp.png')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%生成词云图\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "documents = [_test.split() for _test in hair.review_body]\n",
    "model = Word2Vec(documents,\n",
    "                 size=200,  # 词向量维度\n",
    "                 min_count=5,  # 词频阈值\n",
    "                 window=5)  # 窗口大小   进行训练\n",
    "\n",
    "#构建字典\n",
    "dictionary = corpora.Dictionary(documents)      #dictionary\n",
    "#进行词向量分析\n",
    "corpus = [dictionary.doc2bow(text) for text in documents]\n",
    "num_topics = 20\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "#准备LDA模型\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=dictionary,\n",
    "                                       num_topics=num_topics,\n",
    "                                       eval_every=None,workers=4)       #\n",
    "#按照相关度提取主题\n",
    "top_topics = lda_model.top_topics(corpus)  # , num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "#打印LDA模型和网页版\n",
    "from pprint import pprint\n",
    "pprint(top_topics)\n",
    "import pyLDAvis.gensim\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.show(vis)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%LDA模型\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}